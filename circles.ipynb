{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Conv1D, Flatten\n",
    "from keras.layers import CuDNNLSTM, GRU, LSTM, ConvLSTM2D, CuDNNGRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"circle.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(913, 824)\n",
      "(255, 255, 255)\n",
      "(255, 127, 39)\n"
     ]
    }
   ],
   "source": [
    "print(image.size)\n",
    "print(image.getpixel((1,1)))\n",
    "print(image.getpixel((500,500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise0</th>\n",
       "      <th>noise1</th>\n",
       "      <th>noise10</th>\n",
       "      <th>noise11</th>\n",
       "      <th>noise12</th>\n",
       "      <th>noise13</th>\n",
       "      <th>noise14</th>\n",
       "      <th>noise2</th>\n",
       "      <th>noise3</th>\n",
       "      <th>noise4</th>\n",
       "      <th>noise5</th>\n",
       "      <th>noise6</th>\n",
       "      <th>noise7</th>\n",
       "      <th>noise8</th>\n",
       "      <th>noise9</th>\n",
       "      <th>target</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>869</td>\n",
       "      <td>384</td>\n",
       "      <td>231</td>\n",
       "      <td>816</td>\n",
       "      <td>533</td>\n",
       "      <td>738</td>\n",
       "      <td>725</td>\n",
       "      <td>743</td>\n",
       "      <td>587</td>\n",
       "      <td>151</td>\n",
       "      <td>128</td>\n",
       "      <td>456</td>\n",
       "      <td>777</td>\n",
       "      <td>722</td>\n",
       "      <td>527</td>\n",
       "      <td>0</td>\n",
       "      <td>672</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233</td>\n",
       "      <td>267</td>\n",
       "      <td>191</td>\n",
       "      <td>793</td>\n",
       "      <td>297</td>\n",
       "      <td>183</td>\n",
       "      <td>804</td>\n",
       "      <td>767</td>\n",
       "      <td>697</td>\n",
       "      <td>616</td>\n",
       "      <td>255</td>\n",
       "      <td>254</td>\n",
       "      <td>552</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>478</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>319</td>\n",
       "      <td>257</td>\n",
       "      <td>224</td>\n",
       "      <td>569</td>\n",
       "      <td>838</td>\n",
       "      <td>260</td>\n",
       "      <td>745</td>\n",
       "      <td>685</td>\n",
       "      <td>519</td>\n",
       "      <td>293</td>\n",
       "      <td>523</td>\n",
       "      <td>909</td>\n",
       "      <td>703</td>\n",
       "      <td>319</td>\n",
       "      <td>338</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>592</td>\n",
       "      <td>111</td>\n",
       "      <td>593</td>\n",
       "      <td>583</td>\n",
       "      <td>732</td>\n",
       "      <td>161</td>\n",
       "      <td>281</td>\n",
       "      <td>109</td>\n",
       "      <td>66</td>\n",
       "      <td>356</td>\n",
       "      <td>821</td>\n",
       "      <td>85</td>\n",
       "      <td>536</td>\n",
       "      <td>700</td>\n",
       "      <td>1</td>\n",
       "      <td>593</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>244</td>\n",
       "      <td>162</td>\n",
       "      <td>733</td>\n",
       "      <td>653</td>\n",
       "      <td>654</td>\n",
       "      <td>42</td>\n",
       "      <td>268</td>\n",
       "      <td>769</td>\n",
       "      <td>31</td>\n",
       "      <td>267</td>\n",
       "      <td>529</td>\n",
       "      <td>665</td>\n",
       "      <td>621</td>\n",
       "      <td>276</td>\n",
       "      <td>669</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>374</td>\n",
       "      <td>380</td>\n",
       "      <td>6</td>\n",
       "      <td>439</td>\n",
       "      <td>396</td>\n",
       "      <td>167</td>\n",
       "      <td>625</td>\n",
       "      <td>176</td>\n",
       "      <td>879</td>\n",
       "      <td>642</td>\n",
       "      <td>635</td>\n",
       "      <td>613</td>\n",
       "      <td>796</td>\n",
       "      <td>216</td>\n",
       "      <td>713</td>\n",
       "      <td>0</td>\n",
       "      <td>650</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>405</td>\n",
       "      <td>355</td>\n",
       "      <td>261</td>\n",
       "      <td>499</td>\n",
       "      <td>664</td>\n",
       "      <td>649</td>\n",
       "      <td>792</td>\n",
       "      <td>908</td>\n",
       "      <td>892</td>\n",
       "      <td>798</td>\n",
       "      <td>287</td>\n",
       "      <td>132</td>\n",
       "      <td>290</td>\n",
       "      <td>89</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>887</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>798</td>\n",
       "      <td>207</td>\n",
       "      <td>725</td>\n",
       "      <td>308</td>\n",
       "      <td>601</td>\n",
       "      <td>332</td>\n",
       "      <td>566</td>\n",
       "      <td>463</td>\n",
       "      <td>317</td>\n",
       "      <td>43</td>\n",
       "      <td>273</td>\n",
       "      <td>391</td>\n",
       "      <td>514</td>\n",
       "      <td>226</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>514</td>\n",
       "      <td>318</td>\n",
       "      <td>786</td>\n",
       "      <td>0</td>\n",
       "      <td>787</td>\n",
       "      <td>538</td>\n",
       "      <td>244</td>\n",
       "      <td>309</td>\n",
       "      <td>383</td>\n",
       "      <td>798</td>\n",
       "      <td>232</td>\n",
       "      <td>643</td>\n",
       "      <td>142</td>\n",
       "      <td>203</td>\n",
       "      <td>791</td>\n",
       "      <td>1</td>\n",
       "      <td>309</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>280</td>\n",
       "      <td>192</td>\n",
       "      <td>507</td>\n",
       "      <td>328</td>\n",
       "      <td>675</td>\n",
       "      <td>766</td>\n",
       "      <td>76</td>\n",
       "      <td>489</td>\n",
       "      <td>89</td>\n",
       "      <td>492</td>\n",
       "      <td>675</td>\n",
       "      <td>648</td>\n",
       "      <td>79</td>\n",
       "      <td>195</td>\n",
       "      <td>477</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>403</td>\n",
       "      <td>104</td>\n",
       "      <td>355</td>\n",
       "      <td>323</td>\n",
       "      <td>859</td>\n",
       "      <td>254</td>\n",
       "      <td>120</td>\n",
       "      <td>391</td>\n",
       "      <td>700</td>\n",
       "      <td>356</td>\n",
       "      <td>506</td>\n",
       "      <td>790</td>\n",
       "      <td>477</td>\n",
       "      <td>614</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>555</td>\n",
       "      <td>93</td>\n",
       "      <td>506</td>\n",
       "      <td>58</td>\n",
       "      <td>332</td>\n",
       "      <td>590</td>\n",
       "      <td>428</td>\n",
       "      <td>331</td>\n",
       "      <td>213</td>\n",
       "      <td>191</td>\n",
       "      <td>29</td>\n",
       "      <td>185</td>\n",
       "      <td>414</td>\n",
       "      <td>481</td>\n",
       "      <td>758</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>95</td>\n",
       "      <td>559</td>\n",
       "      <td>172</td>\n",
       "      <td>242</td>\n",
       "      <td>77</td>\n",
       "      <td>387</td>\n",
       "      <td>671</td>\n",
       "      <td>883</td>\n",
       "      <td>602</td>\n",
       "      <td>574</td>\n",
       "      <td>107</td>\n",
       "      <td>443</td>\n",
       "      <td>574</td>\n",
       "      <td>826</td>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>322</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>700</td>\n",
       "      <td>679</td>\n",
       "      <td>712</td>\n",
       "      <td>14</td>\n",
       "      <td>601</td>\n",
       "      <td>841</td>\n",
       "      <td>734</td>\n",
       "      <td>249</td>\n",
       "      <td>106</td>\n",
       "      <td>299</td>\n",
       "      <td>65</td>\n",
       "      <td>353</td>\n",
       "      <td>689</td>\n",
       "      <td>46</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>640</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>344</td>\n",
       "      <td>814</td>\n",
       "      <td>910</td>\n",
       "      <td>17</td>\n",
       "      <td>89</td>\n",
       "      <td>449</td>\n",
       "      <td>206</td>\n",
       "      <td>411</td>\n",
       "      <td>526</td>\n",
       "      <td>357</td>\n",
       "      <td>235</td>\n",
       "      <td>165</td>\n",
       "      <td>425</td>\n",
       "      <td>384</td>\n",
       "      <td>744</td>\n",
       "      <td>1</td>\n",
       "      <td>578</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>646</td>\n",
       "      <td>637</td>\n",
       "      <td>344</td>\n",
       "      <td>334</td>\n",
       "      <td>200</td>\n",
       "      <td>358</td>\n",
       "      <td>406</td>\n",
       "      <td>136</td>\n",
       "      <td>201</td>\n",
       "      <td>716</td>\n",
       "      <td>441</td>\n",
       "      <td>171</td>\n",
       "      <td>9</td>\n",
       "      <td>132</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>576</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>246</td>\n",
       "      <td>498</td>\n",
       "      <td>12</td>\n",
       "      <td>79</td>\n",
       "      <td>341</td>\n",
       "      <td>334</td>\n",
       "      <td>600</td>\n",
       "      <td>233</td>\n",
       "      <td>310</td>\n",
       "      <td>590</td>\n",
       "      <td>747</td>\n",
       "      <td>338</td>\n",
       "      <td>691</td>\n",
       "      <td>142</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>396</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>835</td>\n",
       "      <td>525</td>\n",
       "      <td>864</td>\n",
       "      <td>579</td>\n",
       "      <td>457</td>\n",
       "      <td>728</td>\n",
       "      <td>259</td>\n",
       "      <td>620</td>\n",
       "      <td>432</td>\n",
       "      <td>884</td>\n",
       "      <td>777</td>\n",
       "      <td>743</td>\n",
       "      <td>520</td>\n",
       "      <td>372</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>862</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>291</td>\n",
       "      <td>408</td>\n",
       "      <td>334</td>\n",
       "      <td>801</td>\n",
       "      <td>239</td>\n",
       "      <td>19</td>\n",
       "      <td>742</td>\n",
       "      <td>610</td>\n",
       "      <td>540</td>\n",
       "      <td>11</td>\n",
       "      <td>595</td>\n",
       "      <td>301</td>\n",
       "      <td>270</td>\n",
       "      <td>405</td>\n",
       "      <td>692</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>743</td>\n",
       "      <td>253</td>\n",
       "      <td>534</td>\n",
       "      <td>322</td>\n",
       "      <td>732</td>\n",
       "      <td>99</td>\n",
       "      <td>113</td>\n",
       "      <td>385</td>\n",
       "      <td>519</td>\n",
       "      <td>326</td>\n",
       "      <td>260</td>\n",
       "      <td>245</td>\n",
       "      <td>437</td>\n",
       "      <td>272</td>\n",
       "      <td>460</td>\n",
       "      <td>0</td>\n",
       "      <td>553</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    noise0  noise1  noise10  noise11  noise12  noise13  noise14  noise2  \\\n",
       "0      869     384      231      816      533      738      725     743   \n",
       "1      233     267      191      793      297      183      804     767   \n",
       "2      319     257      224      569      838      260      745     685   \n",
       "3       96     592      111      593      583      732      161     281   \n",
       "4      244     162      733      653      654       42      268     769   \n",
       "5      374     380        6      439      396      167      625     176   \n",
       "6      405     355      261      499      664      649      792     908   \n",
       "7      798     207      725      308      601      332      566     463   \n",
       "8      514     318      786        0      787      538      244     309   \n",
       "9      280     192      507      328      675      766       76     489   \n",
       "10     403     104      355      323      859      254      120     391   \n",
       "11     555      93      506       58      332      590      428     331   \n",
       "12      95     559      172      242       77      387      671     883   \n",
       "13     700     679      712       14      601      841      734     249   \n",
       "14     344     814      910       17       89      449      206     411   \n",
       "15     646     637      344      334      200      358      406     136   \n",
       "16     246     498       12       79      341      334      600     233   \n",
       "17     835     525      864      579      457      728      259     620   \n",
       "18     291     408      334      801      239       19      742     610   \n",
       "19     743     253      534      322      732       99      113     385   \n",
       "\n",
       "    noise3  noise4  noise5  noise6  noise7  noise8  noise9  target    x    y  \n",
       "0      587     151     128     456     777     722     527       0  672  163  \n",
       "1      697     616     255     254     552      80      65       1  478  424  \n",
       "2      519     293     523     909     703     319     338       0  254  517  \n",
       "3      109      66     356     821      85     536     700       1  593  320  \n",
       "4       31     267     529     665     621     276     669       0   92  424  \n",
       "5      879     642     635     613     796     216     713       0  650  821  \n",
       "6      892     798     287     132     290      89     258       0  887  675  \n",
       "7      317      43     273     391     514     226     381       1  359  375  \n",
       "8      383     798     232     643     142     203     791       1  309  359  \n",
       "9       89     492     675     648      79     195     477       0   53  304  \n",
       "10     700     356     506     790     477     614      34       0  318  241  \n",
       "11     213     191      29     185     414     481     758       0  138   17  \n",
       "12     602     574     107     443     574     826     871       0  322  159  \n",
       "13     106     299      65     353     689      46     219       0  640  155  \n",
       "14     526     357     235     165     425     384     744       1  578  364  \n",
       "15     201     716     441     171       9     132     106       1  576  443  \n",
       "16     310     590     747     338     691     142     232       1  396  287  \n",
       "17     432     884     777     743     520     372     228       0  862  274  \n",
       "18     540      11     595     301     270     405     692       0  167  578  \n",
       "19     519     326     260     245     437     272     460       0  553  802  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate data\n",
    "\n",
    "rows_list = list()\n",
    "noise = list()\n",
    "noises = 15\n",
    "for i in range(10000):\n",
    "    rand_x = np.random.randint(image.size[0])\n",
    "    rand_y = np.random.randint(image.size[1])\n",
    "    noise_dict = dict()\n",
    "    for n in range(noises):\n",
    "        noise_dict[\"noise%s\"%n] = np.random.randint(image.size[np.random.randint(len(image.size))])\n",
    "    if image.getpixel((rand_x, rand_y)) == (255, 127, 39):  # ms orange lul\n",
    "        noise_dict.update({\"x\":rand_x, \"y\":rand_y, \"target\":1})\n",
    "    else:\n",
    "        noise_dict.update({\"x\":rand_x, \"y\":rand_y, \"target\":0})\n",
    "    rows_list.append(noise_dict)\n",
    "\n",
    "df = pd.DataFrame(rows_list)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, validation_data=None):\n",
    "    data = data.convert_objects(convert_numeric=True)\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    train_in = data.drop(columns=[\"target\"])\n",
    "    train_out = data[\"target\"]\n",
    "    \n",
    "    traini_np = train_in.values\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    train_in = scaler.fit_transform(traini_np)\n",
    "    \n",
    "    if validation_data is None:\n",
    "        return train_in, train_out\n",
    "    else:\n",
    "        validation_data = validation_data.convert_objects(convert_numeric=True)\n",
    "        validation_data = validation_data.fillna(0)\n",
    "        \n",
    "        val_np = validation_data.values\n",
    "        val = scaler.transform(val_np)\n",
    "        return train_in, train_out, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "features, target = preprocess(df)\n",
    "t_in, v_in, t_out, v_out = train_test_split(features, target, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4,\n",
       "                       oob_score=False, random_state=0, verbose=1,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, random_state=0, n_jobs=4, verbose=1)\n",
    "rf.fit(t_in, t_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_printer(pred, actual):\n",
    "    assert len(pred) == len(actual)\n",
    "    fp=0;tp=0;fn=0;tn=0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i]==actual[i]==1:\n",
    "            tp+=1\n",
    "        elif pred[i]==actual[i]==0:\n",
    "            tn+=1\n",
    "        elif 1==pred[i]!=actual[i]:\n",
    "            fp+=1\n",
    "        elif 0==pred[i]!=actual[i]:\n",
    "            fn+=1\n",
    "        else:\n",
    "            raise Exception(\"wtf\")\n",
    "    print(\"Stats:\\nCorrect true: %s\\nCorrect false: %s\\nFalse positive: %s\\nFalse negative: %s\\n\" % (tp, tn, fp, fn))\n",
    "    \n",
    "def add_useless_dimension(in_array):\n",
    "    new_shape = list(np.shape(in_array))\n",
    "    new_shape.append(1)\n",
    "    in_array = np.array(in_array)\n",
    "    return in_array.reshape(tuple(new_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:\n",
      "Correct true: 338\n",
      "Correct false: 1625\n",
      "False positive: 27\n",
      "False negative: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sols = rf.predict(v_in)\n",
    "acc_printer(sols, v_out.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 17)\n",
      "(8000,)\n",
      "(2000, 17)\n",
      "(2000,)\n",
      "Tensor(\"x_input_24:0\", shape=(None, 17, 1), dtype=float32)\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x_input (InputLayer)         (None, 17, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 17, 64)            128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 17, 64)            256       \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 17, 64)            24768     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 17, 32)            9312      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 16)                2352      \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 40,017\n",
      "Trainable params: 39,889\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 5s 679us/step - loss: 0.3842 - accuracy: 0.8436 - val_loss: 1.1354 - val_accuracy: 0.1740\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.13539, saving model to models/lul.h5\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 4s 473us/step - loss: 0.1025 - accuracy: 0.9654 - val_loss: 2.2510 - val_accuracy: 0.1770\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.13539\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 4s 466us/step - loss: 0.0644 - accuracy: 0.9791 - val_loss: 2.0248 - val_accuracy: 0.3310\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.13539\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 4s 473us/step - loss: 0.0551 - accuracy: 0.9808 - val_loss: 0.4596 - val_accuracy: 0.8330\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.13539 to 0.45955, saving model to models/lul.h5\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 4s 489us/step - loss: 0.0530 - accuracy: 0.9808 - val_loss: 0.2793 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45955 to 0.27929, saving model to models/lul.h5\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 4s 472us/step - loss: 0.0459 - accuracy: 0.9843 - val_loss: 0.1350 - val_accuracy: 0.9485\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27929 to 0.13498, saving model to models/lul.h5\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 4s 471us/step - loss: 0.0405 - accuracy: 0.9865 - val_loss: 0.0415 - val_accuracy: 0.9830\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.13498 to 0.04151, saving model to models/lul.h5\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 4s 479us/step - loss: 0.0394 - accuracy: 0.9859 - val_loss: 0.0333 - val_accuracy: 0.9875\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04151 to 0.03326, saving model to models/lul.h5\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 4s 485us/step - loss: 0.0383 - accuracy: 0.9859 - val_loss: 0.0402 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03326\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 4s 477us/step - loss: 0.0312 - accuracy: 0.9889 - val_loss: 0.0556 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03326\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 4s 487us/step - loss: 0.0438 - accuracy: 0.9834 - val_loss: 0.0387 - val_accuracy: 0.9840\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03326\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 4s 480us/step - loss: 0.0311 - accuracy: 0.9883 - val_loss: 0.0262 - val_accuracy: 0.9885\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03326 to 0.02616, saving model to models/lul.h5\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 4s 482us/step - loss: 0.0340 - accuracy: 0.9868 - val_loss: 0.0362 - val_accuracy: 0.9860\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02616\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 4s 489us/step - loss: 0.0383 - accuracy: 0.9851 - val_loss: 0.0432 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02616\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 4s 484us/step - loss: 0.0390 - accuracy: 0.9837 - val_loss: 0.0475 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02616\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 0.0290 - accuracy: 0.9894 - val_loss: 0.0270 - val_accuracy: 0.9915\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02616\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 4s 485us/step - loss: 0.0297 - accuracy: 0.9877 - val_loss: 0.0218 - val_accuracy: 0.9910\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02616 to 0.02183, saving model to models/lul.h5\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 4s 475us/step - loss: 0.0350 - accuracy: 0.9859 - val_loss: 0.0263 - val_accuracy: 0.9905\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02183\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 4s 468us/step - loss: 0.0279 - accuracy: 0.9896 - val_loss: 0.0385 - val_accuracy: 0.9810\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02183\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 4s 463us/step - loss: 0.0286 - accuracy: 0.9896 - val_loss: 0.0308 - val_accuracy: 0.9875\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02183\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 4s 471us/step - loss: 0.0280 - accuracy: 0.9891 - val_loss: 0.0554 - val_accuracy: 0.9780\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02183\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 4s 474us/step - loss: 0.0243 - accuracy: 0.9904 - val_loss: 0.0603 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02183\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 4s 479us/step - loss: 0.0299 - accuracy: 0.9890 - val_loss: 0.0288 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02183\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 4s 467us/step - loss: 0.0318 - accuracy: 0.9874 - val_loss: 0.0253 - val_accuracy: 0.9870\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02183\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 4s 468us/step - loss: 0.0262 - accuracy: 0.9898 - val_loss: 0.0338 - val_accuracy: 0.9855\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02183\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 4s 478us/step - loss: 0.0291 - accuracy: 0.9881 - val_loss: 0.0267 - val_accuracy: 0.9900\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02183\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 4s 476us/step - loss: 0.0271 - accuracy: 0.9889 - val_loss: 0.0301 - val_accuracy: 0.9870\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02183\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 4s 525us/step - loss: 0.0260 - accuracy: 0.9896 - val_loss: 0.0574 - val_accuracy: 0.9780\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02183\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 4s 540us/step - loss: 0.0316 - accuracy: 0.9862 - val_loss: 0.0512 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02183\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 4s 548us/step - loss: 0.0232 - accuracy: 0.9911 - val_loss: 0.0533 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02183\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 4s 538us/step - loss: 0.0266 - accuracy: 0.9905 - val_loss: 0.0916 - val_accuracy: 0.9745\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02183\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 4s 503us/step - loss: 0.0362 - accuracy: 0.9849 - val_loss: 0.0332 - val_accuracy: 0.9850\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02183\n"
     ]
    }
   ],
   "source": [
    "def cnn_gru_model(data):\n",
    "    if len(np.shape(data[\"x_train\"])) < 3:\n",
    "        data[\"x_train\"] = add_useless_dimension(data[\"x_train\"])\n",
    "    input_x = keras.Input(shape=(np.shape(data['x_train'])[1], 1), name=\"x_input\")\n",
    "    print(input_x)\n",
    "    \n",
    "    conv = (Conv1D(64, 1, activation='relu'))(input_x)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    gru = (GRU(64, return_sequences=True))(conv)\n",
    "    gru = (GRU(32, return_sequences=True))(gru)\n",
    "    gru = (GRU(16, return_sequences=False))(gru)\n",
    "    \n",
    "    gru = layers.Dense(64, activation='sigmoid')(gru)\n",
    "    gru = layers.Dense(32, activation='sigmoid')(gru)\n",
    "    y = layers.Dense(1, activation='sigmoid')(gru)\n",
    "    \n",
    "    #model\n",
    "    #classifier = Model([input_x, input_xd], y)\n",
    "\n",
    "    classifier = Model([input_x], y)\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "def cnn_lstm_model(data):\n",
    "    if len(np.shape(data[\"x_train\"])) < 3:\n",
    "        data[\"x_train\"] = add_useless_dimension(data[\"x_train\"])\n",
    "    input_x = keras.Input(shape=(np.shape(data['x_train'])[1], 1), name=\"x_input\")\n",
    "    \n",
    "    conv = (Conv1D(64, 1, activation='relu'))(input_x)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = (Conv1D(64, 2, activation='relu'))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = (Conv1D(64, 3, activation='relu'))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "            \n",
    "    lstm = (LSTM(64, return_sequences=True))(conv)\n",
    "    lstm = (LSTM(64, return_sequences=True))(lstm)\n",
    "    lstm = (LSTM(64, return_sequences=False))(lstm)\n",
    "\n",
    "    #output layer\n",
    "    lstm = layers.Dense(64, activation='relu')(lstm)\n",
    "    lstm = layers.Dense(32, activation='relu')(lstm)\n",
    "    y = layers.Dense(1, activation='sigmoid')(lstm)\n",
    "    \n",
    "    classifier = Model([input_x], y)\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "def train_model(model, t_in, t_out, v_in, v_out, MODEL_NAME, class_weight=None):\n",
    "    if len(np.shape(t_in)) < 3:\n",
    "        t_in = add_useless_dimension(t_in)\n",
    "        v_in = add_useless_dimension(v_in)\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=15),\n",
    "             ModelCheckpoint(filepath=\"models/\"+MODEL_NAME+\".h5\", monitor='val_loss', verbose=2, save_best_only=True)]\n",
    "    model.summary()\n",
    "    model.fit(x={\"x_input\": t_in}, y=t_out, shuffle=True, epochs=100, callbacks=callbacks, batch_size=64,\n",
    "              verbose=1, validation_data=([v_in], v_out), class_weight=class_weight)\n",
    "    return model\n",
    "\n",
    "print(np.shape(t_in))\n",
    "print(np.shape(t_out))\n",
    "print(np.shape(v_in))\n",
    "print(np.shape(v_out))\n",
    "network = cnn_gru_model({'x_train': t_in})\n",
    "model = train_model(network, t_in, t_out, v_in, v_out, \"lul\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:\n",
      "Correct true: 342\n",
      "Correct false: 1628\n",
      "False positive: 24\n",
      "False negative: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sols = model.predict(add_useless_dimension(v_in))\n",
    "acc_printer(np.round(sols), v_out.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.np_utils.to_categorical([1,1,0,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
